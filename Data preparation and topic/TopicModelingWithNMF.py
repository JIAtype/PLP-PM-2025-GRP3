import json
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
import nltk
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords

# ä¸‹è½½ NLTK å¿…éœ€çš„èµ„æº
nltk.download('punkt')
nltk.download('stopwords')

class TopicModeling:
    def __init__(self, json_file, num_topics=5):
        """
        åˆå§‹åŒ– TopicModeling ç±»ï¼Œæ¥æ”¶ JSON æ–‡ä»¶è·¯å¾„å’Œä¸»é¢˜æ•°é‡ã€‚
        :param json_file: è¾“å…¥çš„ JSON æ–‡ä»¶è·¯å¾„
        :param num_topics: æå–çš„ä¸»é¢˜æ•°
        """
        self.json_file = json_file
        self.num_topics = num_topics
        self.data = self.load_data()

    def load_data(self):
        """
        åŠ è½½ JSON æ•°æ®ã€‚
        :return: åŠ è½½çš„ JSON æ•°æ®
        """
        try:
            with open(self.json_file, "r", encoding="utf-8") as file:
                data = json.load(file)
            return data
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½ JSON æ–‡ä»¶: {e}")
            return {}

    def extract_captions(self):
        """
        æå–æ‰€æœ‰è§†é¢‘çš„å­—å¹•æ–‡æœ¬ã€‚
        :return: å­—å¹•åˆ—è¡¨å’Œè§†é¢‘ ID æ˜ å°„
        """
        captions_list = []
        video_id_map = []  # å­˜å‚¨ (author, playlist_id, video_id)

        for author, playlists in self.data.items():
            for playlist_id, videos in playlists.items():
                for video_id, video_data in videos.items():
                    captions = video_data.get("captions", "").strip()  # è¯»å–å­—å¹•
                    if captions:
                        captions_list.append(captions)
                        video_id_map.append((author, playlist_id, video_id))

        return captions_list, video_id_map

    def topic_modeling(self, captions_list):
        """
        ä½¿ç”¨ NMF è¿›è¡Œä¸»é¢˜å»ºæ¨¡ã€‚
        :param captions_list: æå–çš„å­—å¹•æ–‡æœ¬åˆ—è¡¨
        :return: ä¸»é¢˜å…³é”®è¯
        """
        # è¿›è¡Œæ–‡æœ¬å‘é‡åŒ–
        vectorizer = TfidfVectorizer(stop_words="english", max_features=1000)
        X = vectorizer.fit_transform(captions_list)

        # è®­ç»ƒ NMF æ¨¡å‹
        nmf_model = NMF(n_components=self.num_topics, random_state=42)
        W = nmf_model.fit_transform(X)
        H = nmf_model.components_

        # è·å–ä¸»é¢˜å…³é”®è¯
        terms = vectorizer.get_feature_names_out()
        topic_keywords = [" ".join([terms[i] for i in topic.argsort()[-10:]]) for topic in H]

        return topic_keywords, W

    def assign_topics(self, video_id_map, W, topic_keywords):
        """
        å°†ä¸»é¢˜åˆ†é…ç»™æ¯ä¸ªè§†é¢‘ã€‚
        :param video_id_map: å­˜å‚¨ (author, playlist_id, video_id) çš„åˆ—è¡¨
        :param W: ä¸»é¢˜åˆ†é…çŸ©é˜µ
        :param topic_keywords: ä¸»é¢˜å…³é”®è¯
        """
        for i, (author, playlist_id, video_id) in enumerate(video_id_map):
            best_topic = W[i].argmax()
            self.data[author][playlist_id][video_id]["topic"] = topic_keywords[best_topic]  # ç›´æ¥å­˜å­—ç¬¦ä¸²

    def save_data(self):
        """
        å°†æ›´æ–°åçš„æ•°æ®å†™å›åˆ° JSON æ–‡ä»¶ã€‚
        """
        try:
            with open(self.json_file, "w", encoding="utf-8") as file:
                json.dump(self.data, file, ensure_ascii=False, indent=4)
            print("ğŸ‰ ä¸»é¢˜å»ºæ¨¡å®Œæˆå¹¶ä¿å­˜åˆ° data.json")
        except Exception as e:
            print(f"âš ï¸ æ— æ³•ä¿å­˜ JSON æ–‡ä»¶: {e}")

    def run(self):
        """
        è¿è¡Œæ•´ä¸ªä¸»é¢˜å»ºæ¨¡è¿‡ç¨‹ã€‚
        """
        captions_list, video_id_map = self.extract_captions()
        
        if not captions_list:
            print("No captions found in data.json")
            return
        
        topic_keywords, W = self.topic_modeling(captions_list)
        self.assign_topics(video_id_map, W, topic_keywords)
        self.save_data()

if __name__ == "__main__":
    # è¿™é‡Œä¼ å…¥æ•°æ®æ–‡ä»¶çš„è·¯å¾„ï¼Œå’Œä½ æƒ³è¦æå–çš„ä¸»é¢˜æ•°
    topic_modeler = TopicModeling("data.json", num_topics=5)
    topic_modeler.run()