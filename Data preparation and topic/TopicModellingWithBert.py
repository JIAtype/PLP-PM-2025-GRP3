import json
from transformers import BertTokenizer, BertModel
import torch
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import CountVectorizer
import nltk
from nltk.tokenize import sent_tokenize
import numpy as np

# ä¸‹è½½ NLTK å¿…éœ€çš„èµ„æº
nltk.download('punkt')

class TopicModelingBERT:
    def __init__(self, json_file, num_topics=5):
        """
        åˆå§‹åŒ– TopicModelingBERT ç±»ï¼Œæ¥æ”¶ JSON æ–‡ä»¶è·¯å¾„å’Œä¸»é¢˜æ•°é‡ã€‚
        :param json_file: è¾“å…¥çš„ JSON æ–‡ä»¶è·¯å¾„
        :param num_topics: æå–çš„ä¸»é¢˜æ•°
        """
        self.json_file = json_file
        self.num_topics = num_topics
        self.data = self.load_data()

        # åŠ è½½ BERT tokenizer å’Œæ¨¡å‹
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        self.model = BertModel.from_pretrained('bert-base-uncased')

    def load_data(self):
        """
        åŠ è½½ JSON æ•°æ®ã€‚
        :return: åŠ è½½çš„ JSON æ•°æ®
        """
        try:
            with open(self.json_file, "r", encoding="utf-8") as file:
                data = json.load(file)
            return data
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½ JSON æ–‡ä»¶: {e}")
            return {}

    def extract_captions(self):
        """
        æå–æ‰€æœ‰è§†é¢‘çš„å­—å¹•æ–‡æœ¬ã€‚
        :return: å­—å¹•åˆ—è¡¨å’Œè§†é¢‘ ID æ˜ å°„
        """
        captions_list = []
        video_id_map = []  # å­˜å‚¨ (author, playlist_id, video_id)

        for author, playlists in self.data.items():
            for playlist_id, videos in playlists.items():
                for video_id, video_data in videos.items():
                    captions = video_data.get("captions", "").strip()  # è¯»å–å­—å¹•
                    if captions:
                        sentences = sent_tokenize(captions)  # æŒ‰å¥å­æ‹†åˆ†
                        captions_list.extend(sentences)
                        for _ in sentences:
                            video_id_map.append((author, playlist_id, video_id))

        return captions_list, video_id_map

    def get_bert_embeddings(self, texts):
        """
        ä½¿ç”¨ BERT æå–æ–‡æœ¬ç‰¹å¾ï¼ˆåµŒå…¥å‘é‡ï¼‰ã€‚
        :param texts: è¾“å…¥çš„æ–‡æœ¬åˆ—è¡¨
        :return: BERT æå–çš„åµŒå…¥å‘é‡
        """
        inputs = self.tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=512)
        with torch.no_grad():
            outputs = self.model(**inputs)
        embeddings = outputs.last_hidden_state.mean(dim=1).numpy()  # ä½¿ç”¨å¹³å‡æ± åŒ–å¾—åˆ°å¥å­çš„å‘é‡
        return embeddings

    def topic_modeling(self, captions_list):
        """
        ä½¿ç”¨ KMeans è¿›è¡Œä¸»é¢˜å»ºæ¨¡ã€‚
        :param captions_list: æå–çš„å­—å¹•æ–‡æœ¬åˆ—è¡¨
        :return: èšç±»æ ‡ç­¾å’Œ BERT åµŒå…¥å‘é‡
        """
        # è·å–æ‰€æœ‰å¥å­çš„ BERT åµŒå…¥å‘é‡
        embeddings = self.get_bert_embeddings(captions_list)

        # ä½¿ç”¨ KMeans èšç±»è¿›è¡Œä¸»é¢˜å»ºæ¨¡
        kmeans = KMeans(n_clusters=self.num_topics, random_state=42)
        kmeans.fit(embeddings)

        return kmeans.labels_, embeddings

    def get_top_keywords_for_topics(self, captions, labels, num_keywords=5):
        """
        è·å–æ¯ä¸ªä¸»é¢˜çš„å…³é”®è¯ï¼ˆæœ€å…·ä»£è¡¨æ€§çš„è¯ï¼‰ã€‚
        :param captions: è¾“å…¥çš„å­—å¹•åˆ—è¡¨
        :param labels: èšç±»æ ‡ç­¾
        :param num_keywords: æ¯ä¸ªä¸»é¢˜æå–çš„å…³é”®è¯æ•°
        :return: æ¯ä¸ªä¸»é¢˜çš„å…³é”®è¯
        """
        vectorizer = CountVectorizer(stop_words='english')
        X = vectorizer.fit_transform(captions)
        feature_names = np.array(vectorizer.get_feature_names_out())

        topic_keywords = {}
        for topic_num in range(self.num_topics):
            # è·å–å±äºè¯¥ä¸»é¢˜çš„æ‰€æœ‰å¥å­
            topic_indices = np.where(labels == topic_num)[0]
            topic_sentences = [captions[i] for i in topic_indices]

            # è®¡ç®—è¿™äº›å¥å­çš„è¯é¢‘
            topic_matrix = X[topic_indices]
            word_freq = topic_matrix.sum(axis=0).A1  # è·å–è¯é¢‘
            sorted_indices = word_freq.argsort()[::-1]  # æ’åº

            # æå–å‰ num_keywords ä¸ªå…³é”®è¯
            top_keywords = feature_names[sorted_indices][:num_keywords]
            topic_keywords[topic_num] = top_keywords

        return topic_keywords

    def assign_topics(self, video_id_map, kmeans_labels, topic_keywords):
        """
        å°†ä¸»é¢˜åˆ†é…ç»™æ¯ä¸ªè§†é¢‘ã€‚
        :param video_id_map: å­˜å‚¨ (author, playlist_id, video_id) çš„åˆ—è¡¨
        :param kmeans_labels: èšç±»æ ‡ç­¾
        :param topic_keywords: ä¸»é¢˜å…³é”®è¯
        """
        for i, (author, playlist_id, video_id) in enumerate(video_id_map):
            best_topic = kmeans_labels[i]
            topic_name = " ".join(topic_keywords[best_topic])  # é€šè¿‡å…³é”®è¯ç”Ÿæˆä¸»é¢˜åç§°
            self.data[author][playlist_id][video_id]["topic"] = topic_name

    def save_data(self):
        """
        å°†æ›´æ–°åçš„æ•°æ®ä¿å­˜å› JSON æ–‡ä»¶ã€‚
        """
        try:
            with open(self.json_file, "w", encoding="utf-8") as file:
                json.dump(self.data, file, ensure_ascii=False, indent=4)
            print("ğŸ‰ ä¸»é¢˜å»ºæ¨¡å®Œæˆå¹¶ä¿å­˜åˆ° data.json")
        except Exception as e:
            print(f"âš ï¸ æ— æ³•ä¿å­˜ JSON æ–‡ä»¶: {e}")

    def run(self):
        """
        è¿è¡Œæ•´ä¸ªä¸»é¢˜å»ºæ¨¡è¿‡ç¨‹ã€‚
        """
        captions_list, video_id_map = self.extract_captions()

        if not captions_list:
            print("No captions found in data.json")
            return

        kmeans_labels, embeddings = self.topic_modeling(captions_list)
        topic_keywords = self.get_top_keywords_for_topics(captions_list, kmeans_labels)
        self.assign_topics(video_id_map, kmeans_labels, topic_keywords)
        self.save_data()


if __name__ == "__main__":
    # ä¼ å…¥æ•°æ®æ–‡ä»¶çš„è·¯å¾„ï¼Œå’Œä½ æƒ³è¦æå–çš„ä¸»é¢˜æ•°
    topic_modeler = TopicModelingBERT("data.json", num_topics=5)
    topic_modeler.run()