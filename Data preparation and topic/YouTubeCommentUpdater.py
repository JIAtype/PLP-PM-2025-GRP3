import json
from googleapiclient.discovery import build

class YouTubeCommentExtractor:
    def __init__(self, api_key, json_file):
        """
        åˆå§‹åŒ– YouTubeCommentExtractor ç±»ï¼Œæ¥æ”¶ API å¯†é’¥å’Œ JSON æ–‡ä»¶è·¯å¾„ã€‚
        :param api_key: YouTube API å¯†é’¥
        :param json_file: è¾“å…¥çš„ JSON æ–‡ä»¶è·¯å¾„
        """
        self.api_key = api_key
        self.json_file = json_file
        self.data = self.load_data()

        # åˆå§‹åŒ– YouTube API å®¢æˆ·ç«¯
        self.youtube = build('youtube', 'v3', developerKey=self.api_key)

    def load_data(self):
        """
        åŠ è½½ JSON æ•°æ®ã€‚
        :return: åŠ è½½çš„ JSON æ•°æ®
        """
        try:
            with open(self.json_file, "r", encoding="utf-8") as file:
                data = json.load(file)
            return data
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½ JSON æ–‡ä»¶: {e}")
            return {}

    def get_comments_for_video(self, video_id):
        """
        è·å–æŒ‡å®šè§†é¢‘çš„æ‰€æœ‰è¯„è®ºã€‚
        :param video_id: è§†é¢‘ ID
        :return: æ‰€æœ‰è¯„è®ºçš„åˆ—è¡¨
        """
        all_comments = []
        next_page_token = None

        # å¾ªç¯è·å–æ‰€æœ‰è¯„è®º
        while True:
            comment_request = self.youtube.commentThreads().list(
                part="snippet",
                videoId=video_id,
                pageToken=next_page_token,
                textFormat="plainText",
                maxResults=100  # æ¯æ¬¡æœ€å¤šè¿”å› 100 æ¡è¯„è®º
            )
            comment_response = comment_request.execute()

            for item in comment_response['items']:
                top_comment = item['snippet']['topLevelComment']['snippet']
                all_comments.append({
                    'Timestamp': top_comment['publishedAt'],
                    'Comment': top_comment['textDisplay'],
                    'VideoID': video_id
                })

            # è·å–ä¸‹ä¸€é¡µçš„ Tokenï¼Œå¦‚æœæ²¡æœ‰å°±ç»“æŸå¾ªç¯
            next_page_token = comment_response.get('nextPageToken')
            if not next_page_token:
                break
        
        return all_comments

    def extract_comments(self):
        """
        æå–æ‰€æœ‰è§†é¢‘çš„è¯„è®ºå¹¶å°†å…¶æ·»åŠ åˆ° JSON æ•°æ®ä¸­ã€‚
        """
        for author, playlists in self.data.items():
            for playlist_id, playlist_data in playlists.items():
                for video_id, video_data in playlist_data.items():
                    print(f"æ­£åœ¨æå–è§†é¢‘ {video_id} çš„è¯„è®º... ä½œè€…: {author}, æ’­æ”¾åˆ—è¡¨: {playlist_id}")
                    
                    # è·å–è§†é¢‘è¯„è®º
                    video_comments = self.get_comments_for_video(video_id)
                    
                    # å°†è¯„è®ºæ·»åŠ åˆ° JSON æ•°æ®çš„ comments å­—æ®µ
                    video_data["comments"] = [{'Timestamp': comment['Timestamp'], 'Comment': comment['Comment']} for comment in video_comments]

    def save_data(self):
        """
        å°†æ›´æ–°åçš„æ•°æ®ä¿å­˜å› JSON æ–‡ä»¶ã€‚
        """
        try:
            with open(self.json_file, "w", encoding="utf-8") as file:
                json.dump(self.data, file, ensure_ascii=False, indent=4)
            print("ğŸ‰ è¯„è®ºæå–å¹¶æˆåŠŸä¿å­˜åˆ° data.json!")
        except Exception as e:
            print(f"âš ï¸ æ— æ³•ä¿å­˜ JSON æ–‡ä»¶: {e}")

    def run(self):
        """
        è¿è¡Œæ•´ä¸ªè¯„è®ºæå–æµç¨‹ã€‚
        """
        self.extract_comments()
        self.save_data()


if __name__ == "__main__":
    # è¾“å…¥ YouTube API å¯†é’¥å’Œ JSON æ–‡ä»¶è·¯å¾„
    api_key = "AIzaSyDFNwUyRKg40gidJuWB55oXtGoP3KMKsPM"
    json_file = "data.json"

    # åˆ›å»º YouTubeCommentExtractor å®ä¾‹å¹¶è¿è¡Œ
    extractor = YouTubeCommentExtractor(api_key, json_file)
    extractor.run()